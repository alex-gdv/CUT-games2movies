{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe have copied and pasted all relevant code into this notebook - that being said, just in case, I have included all the scripts and things I worked on in the /other/ folder in the .zip file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!cd data\n",
    "!wget https://community.dur.ac.uk/hubert.shum/comp42315resit.zip\n",
    "!unzip comp42315resit.zip\n",
    "!mv ./Data/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Videos to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "path = \"./data/\"  \n",
    "path_images = f\"{path}./images/\"\n",
    "\n",
    "def convert_video_to_frames(path_input, path_output, name):\n",
    "    os.makedirs(path_output, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(path_input)\n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        _, frame = cap.read()\n",
    "        cv2.imwrite(f\"{path_output}./{name}_{count+1}.jpg\", frame)\n",
    "        count = count + 1\n",
    "        if (count > (video_length-1)):\n",
    "            cap.release()\n",
    "            break\n",
    "\n",
    "dirs = [(\"game\", \"trainA\"), (\"movie\", \"trainB\")]\n",
    "for d in dirs:\n",
    "    path_curr = f\"{path}./{d[0]}/\"\n",
    "    files = os.listdir(path_curr)\n",
    "    for f in files:\n",
    "        path_input = f\"{path_curr}./{f}\"\n",
    "        path_output = f\"{path_images}./{d[1]}/\"\n",
    "        print(f\"Current: {path_input}\")\n",
    "        convert_video_to_frames(path_input, path_output, f[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "path_abs = os.getcwd()\n",
    "path_images = \"./data/images/\"\n",
    "SPLIT = 0.3\n",
    "\n",
    "movies = [\"TheGodfather\", \"TheSopranos\", \"TheIrishman\"]\n",
    "files = os.listdir(path_images + \"./trainB/\")\n",
    "for movie in movies:\n",
    "    r = re.compile(f\"{movie}.\")\n",
    "    files2 = list(filter(r.match, files))\n",
    "    num = int(len(files2) * SPLIT)\n",
    "    files2 = sorted(files2, key=lambda s: int(re.findall(r\"\\d+\", s)[-1]))\n",
    "    files2 = files2[-num:]\n",
    "    for f in files2:\n",
    "        p = f\"{path_abs}/data/images/testB/\"\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "        os.rename(f\"{path_abs}/data/images/trainB/{f}\", p + f)\n",
    "\n",
    "files = os.listdir(path_images + \"./trainA/\")\n",
    "num = int(len(files) * SPLIT)\n",
    "files = sorted(files, key=lambda s: int(re.findall(r\"\\d+\", s)[-1]))\n",
    "files = files[-num:]\n",
    "for f in files:\n",
    "    p = f\"{path_abs}/data/images/testA/\"\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    os.rename(f\"{path_abs}/data/images/trainA/{f}\", p + f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "path = \"./data/images/\"\n",
    "path_new = \"./data/images/\"\n",
    "dirs = [\"trainA\", \"trainB\"]\n",
    "for dir in dirs:\n",
    "    files = os.listdir(path + dir + \"/\")\n",
    "    path_temp = path + dir + \"/\"\n",
    "    os.makedirs(path_temp, exist_ok=True)\n",
    "    for f in files:\n",
    "        img = Image.open(path + dir + \"/\" + f)\n",
    "        new_img = T.CenterCrop((512,512))(img)\n",
    "        new_img.save(path_temp + f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/taesungp/contrastive-unpaired-translation.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cloning the git repo, please move the patchL2.py, patchTripletMargin.py, and the modified cut_model.py files to the /models/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/images/ \\\n",
    "    --name g2m --CUT_mode CUT --direction AtoB --load_size 512 \\\n",
    "    --crop_size 512 --preprocess none \\\n",
    "    --n_epochs 50 --n_epochs_decay 50 --num_patches 256\n",
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/images/ \\\n",
    "    --name m2g --CUT_mode CUT --direction BtoA --load_size 512 \\\n",
    "    --crop_size 512 --preprocess none \\\n",
    "    --n_epochs 50 --n_epochs_decay 50 --num_patches 256\n",
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/images/ \\\n",
    "    --name m2g_128 --CUT_mode CUT --direction BtoA --load_size 512 \\\n",
    "    --crop_size 512 --preprocess none \\\n",
    "    --n_epochs 50 --n_epochs_decay 50 --num_patches 128\n",
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/images/ \\\n",
    "    --name g2m_128 --CUT_mode CUT --direction AtoB --load_size 512 \\\n",
    "    --crop_size 512 --preprocess none \\\n",
    "    --n_epochs 50 --n_epochs_decay 50 --num_patches 128\n",
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/images/ \\\n",
    "    --name m2g_64 --CUT_mode CUT --direction BtoA --load_size 512 \\\n",
    "    --crop_size 512 --preprocess none \\\n",
    "    --n_epochs 50 --n_epochs_decay 50 --num_patches 64\n",
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/images/ \\\n",
    "    --name g2m_64 --CUT_mode CUT --direction AtoB --load_size 512 \\\n",
    "    --crop_size 512 --preprocess none \\\n",
    "    --n_epochs 50 --n_epochs_decay 50 --num_patches 64\n",
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/images/ \\\n",
    "    --name m2g_L2 --CUT_mode CUT --direction BtoA --load_size 512 \\\n",
    "    --crop_size 512 --preprocess none \\\n",
    "    --n_epochs 50 --n_epochs_decay 50 --num_patches 256 --patch_L2 True\n",
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/images/ \\\n",
    "    --name g2m_L2 --CUT_mode CUT --direction AtoB --load_size 512 \\\n",
    "    --crop_size 512 --preprocess none \\\n",
    "    --n_epochs 50 --n_epochs_decay 50 --num_patches 256 --patch_L2 True\n",
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/images/ \\\n",
    "    --name m2g_Triplet --CUT_mode CUT --direction BtoA --load_size 512 \\\n",
    "    --crop_size 512 --preprocess none \\\n",
    "    --n_epochs 50 --n_epochs_decay 50 --num_patches 256 --patch_TripletMargin True\n",
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/images/ \\\n",
    "    --name m2g --CUT_mode CUT --direction BtoA --load_size 512 \\\n",
    "    --crop_size 512 --preprocess none \\\n",
    "    --n_epochs 50 --n_epochs_decay 50 --num_patches 256 --patch_TripletMargin True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a footnote, I would like to mention that I considered using the bounding boxes from 3.1 to change the patch sampling strategy by oversampling regions with faces in them. However, due to limited time, I couldn't implement this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Source:\n",
    "# https://pyimagesearch.com/2020/06/15/opencv-fast-fourier-transform-fft-for-blur-detection-in-images-and-video-streams/\n",
    "def detect_blur_fft(image, size=60, thresh=0):\n",
    "    (h, w) = image.shape\n",
    "    (cX, cY) = (int(w / 2.0), int(h / 2.0))\n",
    "    fft = np.fft.fft2(image)\n",
    "    fftShift = np.fft.fftshift(fft)\n",
    "    fftShift[cY - size:cY + size, cX - size:cX + size] = 0\n",
    "    fftShift = np.fft.ifftshift(fftShift)\n",
    "    recon = np.fft.ifft2(fftShift)\n",
    "    magnitude = 20 * np.log(np.abs(recon))\n",
    "    mean = np.mean(magnitude)\n",
    "    return (mean, mean <= thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "path_images = \"./data/images_original/\"\n",
    "path_faces = \"./data/faces/\"\n",
    "path_faces_hq = \"./data/faces_high_quality/\"\n",
    "dirs = [\"trainA\", \"trainB\"]\n",
    "batch_size = 16\n",
    "\n",
    "for d in dirs:\n",
    "    path_input = path_images + d + \"/\"\n",
    "    path_output = path_faces + d + \"/\"\n",
    "    path_output_hq = path_faces_hq + d + \"/\"\n",
    "    os.makedirs(path_output, exist_ok=True)\n",
    "    os.makedirs(path_output_hq, exist_ok=True)\n",
    "    files = os.listdir(path_input)\n",
    "    images = []\n",
    "    for index_files, f in enumerate(files):\n",
    "        image = Image.open(path_input + f)\n",
    "        enhancer = ImageEnhance.Brightness(image)\n",
    "        image = enhancer.enhance(1.5)\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "        if len(images) == batch_size:\n",
    "            face_locations_batch = face_recognition.batch_face_locations(images, batch_size=batch_size)\n",
    "            for index_batch, face_locations in enumerate(face_locations_batch):\n",
    "                index = index_files + 1 - batch_size + index_batch\n",
    "                for index_face, face_location in enumerate(face_locations):\n",
    "                    top, right, bottom, left = face_location\n",
    "                    image_face_np = images[index_batch][top:bottom, left:right]\n",
    "                    image_face = Image.fromarray(image_face_np)\n",
    "                    enhancer = ImageEnhance.Brightness(image_face)\n",
    "                    image_face = enhancer.enhance(0.75)\n",
    "                    image_face = image_face.resize((128,128))\n",
    "                    image_face.save(f\"{path_output}{files[index][:-4]}_{index_face}.jpg\")\n",
    "                    _, blurry = detect_blur_fft(cv2.cvtColor(image_face_np, cv2.COLOR_RGB2GRAY), size=128, thresh=-10)\n",
    "                    if abs(top - bottom) * abs(left - right) > 4096 and not blurry:\n",
    "                        image_face.save(f\"{path_output_hq}{files[index][:-4]}_{index_face}.jpg\")\n",
    "            images = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split faces dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import queue\n",
    "\n",
    "path_train = \"./data/images_faces/\"\n",
    "path_test = \"./data/images_faces_test/\"\n",
    "dirs = [\"trainA\", \"trainB\"]\n",
    "\n",
    "for d in dirs:\n",
    "    path_train_temp = path_train + d + \"/\"\n",
    "    files = os.listdir(path_train_temp)\n",
    "    encodings = []\n",
    "    for f in files:\n",
    "        image = Image.open(path_train_temp + f)\n",
    "        try:\n",
    "            encoding = face_recognition.face_encodings(np.array(image), known_face_locations=[[0,128,128,0]])[0]\n",
    "            encodings.append(encoding)\n",
    "        except IndexError:\n",
    "            print(\"Face not detected.\")\n",
    "\n",
    "    path_temp_test = path_test + d + \"/\"\n",
    "    test_set_indices = []\n",
    "    test_set_size = int(len(files) * 0.3)\n",
    "\n",
    "    def find_close_faces(index, visited_face_indices, num):\n",
    "        new_visited_face_indices = visited_face_indices\n",
    "        new_visited_face_indices += [index]\n",
    "        q = queue.Queue()\n",
    "        q.put(index)\n",
    "        while not q.empty() and len(new_visited_face_indices) < num:\n",
    "            index = q.get()\n",
    "            face_distances = face_recognition.face_distance(encodings[:index] + encodings[index+1:], encodings[index])\n",
    "            list_indices = np.argsort(face_distances)\n",
    "            i = 0\n",
    "            while face_distances[list_indices[i]] <= 0.6 and len(new_visited_face_indices) < num:\n",
    "                if list_indices[i] >= index and (list_indices[i] + 1) not in new_visited_face_indices:\n",
    "                    q.put(list_indices[i]+1)\n",
    "                    new_visited_face_indices += [list_indices[i] + 1]\n",
    "                elif list_indices[i] < index and list_indices[i] not in new_visited_face_indices:\n",
    "                    q.put(list_indices[i])\n",
    "                    new_visited_face_indices += [list_indices[i]]\n",
    "                i += 1\n",
    "        return new_visited_face_indices\n",
    "\n",
    "    while len(test_set_indices) < test_set_size:\n",
    "        index = random.randint(0, len(encodings))\n",
    "        while index in test_set_indices:\n",
    "            index = random.randint(0, len(encodings))\n",
    "        test_set_indices = find_close_faces(index, test_set_indices, test_set_size)\n",
    "    \n",
    "    for i in test_set_indices:\n",
    "        os.makedirs(path_temp_test, exist_ok=True)\n",
    "        os.rename(path_train_temp + files[i], path_temp_test + files[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/images_faces/ \\\n",
    "    --name games2movies_faces --CUT_mode CUT --direction AtoB --load_size 128 \\\n",
    "    --crop_size 128 --preprocess none\n",
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/images_faces/ \\\n",
    "    --name m2g_faces --CUT_mode CUT --direction BtoA --load_size 128 \\\n",
    "    --crop_size 128 --preprocess none\n",
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/faces_high_quality/ \\\n",
    "    --name games2movies_faces_hq --CUT_mode CUT --direction AtoB --load_size 128 \\\n",
    "    --crop_size 128 --preprocess none\n",
    "!python contrastive-unpaired-translation/train.py --dataroot ./data/faces_high_quality/ \\\n",
    "    --name m2g_faces_hq --CUT_mode CUT --direction BtoA --load_size 128 \\\n",
    "    --crop_size 128 --preprocess none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried a bunch of things for this question, none of them worked, except Albumentations. I haven't included my attempts with AttGAN and PA-GAN. the apply_make_up thing I tried below didn't work well, so i left it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import face_recognition\n",
    "import shutil\n",
    "\n",
    "# Source:\n",
    "# https://github.com/ageitgey/face_recognition/blob/master/examples/digital_makeup.py\n",
    "def apply_make_up(image):\n",
    "    face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "    pil_image = Image.fromarray(image)\n",
    "    for face_landmarks in face_landmarks_list:\n",
    "        d = ImageDraw.Draw(pil_image, 'RGBA')\n",
    "\n",
    "        # Make the eyebrows into a nightmare\n",
    "        d.polygon(face_landmarks['left_eyebrow'], fill=(68, 54, 39, 128))\n",
    "        d.polygon(face_landmarks['right_eyebrow'], fill=(68, 54, 39, 128))\n",
    "        d.line(face_landmarks['left_eyebrow'], fill=(68, 54, 39, 150), width=5)\n",
    "        d.line(face_landmarks['right_eyebrow'], fill=(68, 54, 39, 150), width=5)\n",
    "\n",
    "        # Gloss the lips\n",
    "        d.polygon(face_landmarks['top_lip'], fill=(150, 0, 0, 128))\n",
    "        d.polygon(face_landmarks['bottom_lip'], fill=(150, 0, 0, 128))\n",
    "        d.line(face_landmarks['top_lip'], fill=(150, 0, 0, 64), width=8)\n",
    "        d.line(face_landmarks['bottom_lip'], fill=(150, 0, 0, 64), width=8)\n",
    "\n",
    "        # Sparkle the eyes\n",
    "        d.polygon(face_landmarks['left_eye'], fill=(255, 255, 255, 30))\n",
    "        d.polygon(face_landmarks['right_eye'], fill=(255, 255, 255, 30))\n",
    "\n",
    "        # Apply some eyeliner\n",
    "        d.line(face_landmarks['left_eye'] + [face_landmarks['left_eye'][0]], fill=(0, 0, 0, 110), width=6)\n",
    "        d.line(face_landmarks['right_eye'] + [face_landmarks['right_eye'][0]], fill=(0, 0, 0, 110), width=6)\n",
    "\n",
    "    return np.array(pil_image)\n",
    "\n",
    "class MakeUp(A.ImageOnlyTransform):\n",
    "    def apply(self, img, **params):\n",
    "        return apply_make_up(img)\n",
    "\n",
    "# We tested all of Albumentations geometric and photometric transformations\n",
    "# We include only the transformations that result in realistic images\n",
    "transform = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.AdvancedBlur(),\n",
    "        A.GaussianBlur(),\n",
    "        A.MedianBlur(),\n",
    "        A.MotionBlur(),\n",
    "    ]),\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(),\n",
    "        A.MultiplicativeNoise(),\n",
    "        A.ISONoise(),\n",
    "    ]),\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(),\n",
    "        A.RandomToneCurve(),\n",
    "    ]),\n",
    "\n",
    "    A.Emboss(),\n",
    "    A.RingingOvershoot(),\n",
    "    A.Sharpen(),\n",
    "    A.OneOf([\n",
    "        A.CLAHE(),\n",
    "        A.Equalize(),\n",
    "    ]),\n",
    "    A.FancyPCA(),\n",
    "    A.HorizontalFlip(),\n",
    "    A.OneOf([\n",
    "        A.OpticalDistortion(),\n",
    "        A.GridDistortion(),\n",
    "        A.ShiftScaleRotate(),\n",
    "    ]),\n",
    "])\n",
    "\n",
    "def create_augmented_dataset(path_in, path_out, augment_chance=0.3):\n",
    "    files = os.listdir(path_in)\n",
    "    os.makedirs(path_out, exist_ok=True)\n",
    "    for f in files:\n",
    "        shutil.copyfile(path_in + f, path_out + f)\n",
    "        image = cv2.imread(path_in + f)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        augment = random.random() < augment_chance\n",
    "        if augment:\n",
    "            new_image = transform(image=image)[\"image\"]\n",
    "            Image.fromarray(new_image).save(path_out + f[:-4] + \"_a.jpg\")\n",
    "            exit()\n",
    "\n",
    "create_augmented_dataset(\"./data/faces_high_quality_bigger/trainA/\", \"./data/faces_augmented/trainA/\")\n",
    "create_augmented_dataset(\"./data/faces_high_quality_bigger/trainB/\", \"./data/faces_augmented/trainB/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dabbf58f1275fbed79c786f38d53aaa404bb98a684a42de92daa78ba6774d3b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
